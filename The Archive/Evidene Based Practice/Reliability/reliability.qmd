---
title: Reliability
---

Reliability

Reliability is NOT the same as [validity]({{< var ref-validity.path >}})

# Reliability significance

1. One cannot be confident in the results of a scale with poor reliability @streinerReliabilityStatisticsCommentarySeries2016a.

:::{.callout title="Example of Poor reliability"}

- If a person’s score is 10 on 1 occasion and 15 on another (again assuming the person has not changed) @streinerReliabilityStatisticsCommentarySeries2016a
- If 1 rater gives a score of 7 and another a score of 12, we wouldn’t be sure which, if either, score is correct @streinerReliabilityStatisticsCommentarySeries2016a.
- Similarly, if the items of a scale are not related to each other, we would not be sure just what it is that the scale is measuring (assuming that the scale is measuring a homogeneous construct) @streinerReliabilityStatisticsCommentarySeries2016a.
:::

<q>
It would be tantamount to measuring a piece of lumber with a rubber ruler, where each measurement yields a different length. Woe betide the carpenter who tries to cut the wood to fit into an opening. The second reason we want scales to be reliable is that an unreliable scale introduces measurement error, meaning that we would need a larger sample size in order to detect a difference between the groups; for a given difference, the lower the reliability, the larger the necessary sample size to declare the difference to be statistically significant.
</q>

# Misconceptions

Reliability is sometimes used synonymously with ‘precision,’‘agreement,’ and ‘repeatability,’ but these are  misslabeling the concept @streinerReliabilityStatisticsCommentarySeries2016a.



# Types of Reliability

## Interrater Reliability {#interrater}



- INTER‐rater reliability: Consistency of a measure assessed by multiple raters
- Multiple PTs doing the test
- Purpose: Determine the agreement between scores given independently by 2 practitioners @streinerReliabilityStatisticsCommentarySeries2016a

The important point here is the necessity for the independence of the raters; 1 rater must be ignorant of the other’s score. Otherwise, we would have a measure of the degree to which 1 person can accurately copy from another, which tells us much about the raters, but little about the subject.

:::{.callout title="Example of Poor Interrater reliability"}
- If 1 rater gives a score of 7 and another a score of 12, we wouldn’t be sure which, if either, score is correct @streinerReliabilityStatisticsCommentarySeries2016a.
:::

## Intrarater Reliability {#intrarater}

- INTRA‐rater reliability refers to the consistency of a measure assessed by a single rater across multiple time points
- For the same therapist doing the test multiple times

## Test-Retest Reliability

:::{.callout title="Example"}

:::

Test-Retest reliability refers to the similarity of scores when a study participant completes a test at Time 1, and then later at Time 2 @streinerReliabilityStatisticsCommentarySeries2016a.
If this test was reliable, we would expect the 2 scores to be similar, if not identical, on both occasions (assuming that the person has not changed in the interim) @streinerReliabilityStatisticsCommentarySeries2016a.

When assessing test-retest reliability, the length of time between time 1 and 2 creates an issue since there is a fine balance the researchers must find with time since there is trade-off between 2 competing requirements @streinerReliabilityStatisticsCommentarySeries2016a.

- If the duration is too short, the participants will answer the questionnaire by memory in order to appear consistent @streinerReliabilityStatisticsCommentarySeries2016a.
- If the duration is too long, the person's mood, anxiety, symptoms, or whatever we are measuring could change @streinerReliabilityStatisticsCommentarySeries2016a.
- The usual testretest interval is about 2 weeks, but this can be altered based on a number of factors @streinerReliabilityStatisticsCommentarySeries2016a. 
    - The Longer the scale the less time is required between measurements (the person is less likely to remember all of their answers) @streinerReliabilityStatisticsCommentarySeries2016a.
    - Duration is also determined by how quickly the attribute being tested can change @streinerReliabilityStatisticsCommentarySeries2016a.

## Internal Consistency {#internal-consistency}

Internal consistency refers to the degree to which all of the items on a scale are correlated with one another @streinerReliabilityStatisticsCommentarySeries2016a.

- Internal consistency does not measure the patients or cliniciancs using the scale @streinerReliabilityStatisticsCommentarySeries2016a.
- Internal consistency measures how consistem the items of a scale are @streinerReliabilityStatisticsCommentarySeries2016a.

### Significance

- Similarly, if the items of a scale are not related to each other, we would not be sure just what it is that the scale is measuring (assuming that the scale is measuring a homogeneous construct) @streinerReliabilityStatisticsCommentarySeries2016a.

### When to use

When to use internal consistency:

This is important for scales that measure 1 phenomenon:

:::{.callout title="Example"}
such as quality of life or depression
:::

Internal consistency is not appropriate for:

- Indices that measure things that are not expected to be correlated with each other @streinerReliabilityStatisticsCommentarySeries2016a

:::{.callout title="Example"}
The Apgar scale consists of 5 items (heart rate, respiration, muscle tone, reflex response, and skin color) @streinerReliabilityStatisticsCommentarySeries2016a.
These may all occur together in healthy neonates, but in sick children a low score on one item  does not necessarily mean there will be low scores on another item @streinerReliabilityStatisticsCommentarySeries2016a. 
:::

### Measures of Internal Consistency

How to measure internal consistency:

- [Cronbach's Alpha (&alpha;)]({{< var ref-reliability.cronbachs-alpha >}}) 

Interpretation

- You want internal consistency results (i.e. [Cronbach's Alpha (&alpha;)]({{< var ref-reliabiity.cronbachs-alpha >}})) to be high, we do not want it to be too high.
- An (&alpha;) greater than about 0.90 may indicate unnecessary redundancy among the items and means that some of them can be eliminated without jeopardizing the reliability of the scale.

# Reliability Coefficients

Common reliability statistics include:

- [Percent agreement]({{< var ref-reliability.percent-agreement >}})
- [Kappa Coefficient]({{< var ref-reliability.kappa-coefficient >}})
- [Cronbach's Alpha]({{< var ref-reliability.cronbachs-alpha >}})
- [Correlation Coefficient]({{< var ref-reliability.correlation-coefficient >}})
- [Intra-Class Correlation Coefficient]({{< var ref-reliability.icc >}})


## Percent agreement {#percent-agreement}

What percentage of the time 2 different readings of the same thing give you the same result

## Kappa Coefficient (&kappa;) {#kappa-coefficient}
Kappa (Ƙ) ‐ Percent agreement but corrected for chance (chance corrected agreement)

## Cronbach's Alpha (&alpha;) {#cronbachs-alpha}
AKA: Coefficient &alpha;

- Cronbach’s alpha (α) is a measure of the [internal consistency]({{< var ref-reliability.internal-consistency >}}) of multiple items or questions within a single survey
- Used for surveys/questionnaires

Interpretation

- You want internal consistency results (i.e. [Cronbach's Alpha (&alpha;)]({{< var ref-reliabiity.cronbachs-alpha >}})) to be high, we do not want it to be too high.
- An (&alpha;) greater than about 0.90 may indicate unnecessary redundancy among the items and means that some of them can be eliminated without jeopardizing the reliability of the scale.

## Correlation coefficient (r) {#correlation-coefficient}
Correlation coefficient (r )- 
strength of association between diff ratings of the same thing
Only gives info on How closely associated the numbers are (if one increases or decreases, the other does as well)
Does NOT give info on how well those actual values agree (level of agreement?)

## Intra-class Correlation Coefficient (ICC) {#intra-class-correlation-coefficient}
Intra‐class correlation coefficient (ICC)
Most comprehensive measure of reliability since it depends on level of agreement (like kappa) and on the correlation between 2 measures (like correlation coefficient)


# Interpretation

Values range from 0.00 (not reliable) to 1.00 (perfectly reliable) 

| Score | Reliability |
|-|-|
| 0.00 – 0.20 | Poor |
| 0.21 – 0.40 | Fair |
| 0.41 – 0.60 | Moderate |
| 0.61 – 0.80 | Good |
| 0.81 – 1.00 | Excellent |

: "Rule of thumb" Quick interpretation of reliability {.striped .hover}
