---
title: Reliability
---

Reliability

Reliability is NOT the same as [validity]({{< var ref-validity.path >}})

# Reliability significance

1. One cannot be confident in the results of a scale with poor reliability @streinerReliabilityStatisticsCommentarySeries2016a.

:::{.callout title="Example of Poor reliability"}

- If a person’s score is 10 on 1 occasion and 15 on another (again assuming the person has not changed) @streinerReliabilityStatisticsCommentarySeries2016a
- If 1 rater gives a score of 7 and another a score of 12, we wouldn’t be sure which, if either, score is correct @streinerReliabilityStatisticsCommentarySeries2016a.
- Similarly, if the items of a scale are not related to each other, we would not be sure just what it is that the scale is measuring (assuming that the scale is measuring a homogeneous construct) @streinerReliabilityStatisticsCommentarySeries2016a.
:::

<q>
It would be tantamount to measuring a piece of lumber with a rubber ruler, where each measurement yields a different length. Woe betide the carpenter who tries to cut the wood to fit into an opening. The second reason we want scales to be reliable is that an unreliable scale introduces measurement error, meaning that we would need a larger sample size in order to detect a difference between the groups; for a given difference, the lower the reliability, the larger the necessary sample size to declare the difference to be statistically significant.
</q>

# Misconceptions

Reliability is sometimes used synonymously with ‘precision,’‘agreement,’ and ‘repeatability,’ but these are  misslabeling the concept @streinerReliabilityStatisticsCommentarySeries2016a.



# Types of Reliability

## Interrater Reliability {#interrater}



- INTER‐rater reliability: Consistency of a measure assessed by multiple raters
- Multiple PTs doing the test
- Purpose: Determine the agreement between scores given independently by 2 practitioners @streinerReliabilityStatisticsCommentarySeries2016a

The important point here is the necessity for the independence of the raters; 1 rater must be ignorant of the other’s score. Otherwise, we would have a measure of the degree to which 1 person can accurately copy from another, which tells us much about the raters, but little about the subject.

:::{.callout title="Example of Poor Interrater reliability"}
- If 1 rater gives a score of 7 and another a score of 12, we wouldn’t be sure which, if either, score is correct @streinerReliabilityStatisticsCommentarySeries2016a.
:::

:::{.callout title="What to do with poor inter-rater reliability"}

How to improve inter-rater reliability:
- The solution is usually more training of the raters @streinerReliabilityStatisticsCommentarySeries2016a
    - A common strategy is for raters to independently evaluate about 10 people who will not be part of the final sample, and to discuss why they disagreed on specific items; were the criteria unclear, ambiguous, poorly stated, or whatever? 
    - This is repeated on a new group until satisfactory inter-rater reliability is achieved
    - If the study lasts more than a few months, it is usually a good idea to evaluate inter-rater reliability toward the middle to see if there has been any slippage in the level of agreement. 
    - Problems exist with this method if thescale requires interviewing the subject @streinerReliabilityStatisticsCommentarySeries2016a.

:::


## Intrarater Reliability {#intrarater}

- INTRA‐rater reliability refers to the consistency of a measure assessed by a single rater across multiple time points
- For the same therapist doing the test multiple times

## Test-Retest Reliability

:::{.callout title="Example"}

:::

Test-Retest reliability refers to the similarity of scores when a study participant completes a test at Time 1, and then later at Time 2 @streinerReliabilityStatisticsCommentarySeries2016a.
If this test was reliable, we would expect the 2 scores to be similar, if not identical, on both occasions (assuming that the person has not changed in the interim) @streinerReliabilityStatisticsCommentarySeries2016a.

When assessing test-retest reliability, the length of time between time 1 and 2 creates an issue since there is a fine balance the researchers must find with time since there is trade-off between 2 competing requirements @streinerReliabilityStatisticsCommentarySeries2016a.

- If the duration is too short, the participants will answer the questionnaire by memory in order to appear consistent @streinerReliabilityStatisticsCommentarySeries2016a.
- If the duration is too long, the person's mood, anxiety, symptoms, or whatever we are measuring could change @streinerReliabilityStatisticsCommentarySeries2016a.
- The usual testretest interval is about 2 weeks, but this can be altered based on a number of factors @streinerReliabilityStatisticsCommentarySeries2016a. 
    - The Longer the scale the less time is required between measurements (the person is less likely to remember all of their answers) @streinerReliabilityStatisticsCommentarySeries2016a.
    - Duration is also determined by how quickly the attribute being tested can change @streinerReliabilityStatisticsCommentarySeries2016a.

:::{.callout title="What if my scale has low Test-Retest Reliability?"}
- A self administered scale with poor test-retest reliability cannot be changed without altering psychometric validity @streinerReliabilityStatisticsCommentarySeries2016a
- A clinician-administered scale can be slightly changed by insuring that the instructions are clear and understood by the participant @streinerReliabilityStatisticsCommentarySeries2016a
:::

## Internal Consistency {#internal-consistency}

Internal consistency refers to the degree to which all of the items on a scale are correlated with one another @streinerReliabilityStatisticsCommentarySeries2016a.

- Internal consistency does not measure the patients or cliniciancs using the scale @streinerReliabilityStatisticsCommentarySeries2016a.
- Internal consistency measures how consistem the items of a scale are @streinerReliabilityStatisticsCommentarySeries2016a.

### Significance

- Similarly, if the items of a scale are not related to each other, we would not be sure just what it is that the scale is measuring (assuming that the scale is measuring a homogeneous construct) @streinerReliabilityStatisticsCommentarySeries2016a.

### When to use

When to use internal consistency:

This is important for scales that measure 1 phenomenon:

:::{.callout title="Example"}
such as quality of life or depression
:::

Internal consistency is not appropriate for:

- Indices that measure things that are not expected to be correlated with each other @streinerReliabilityStatisticsCommentarySeries2016a

:::{.callout title="Example"}
The Apgar scale consists of 5 items (heart rate, respiration, muscle tone, reflex response, and skin color) @streinerReliabilityStatisticsCommentarySeries2016a.
These may all occur together in healthy neonates, but in sick children a low score on one item  does not necessarily mean there will be low scores on another item @streinerReliabilityStatisticsCommentarySeries2016a. 
:::

### Measures of Internal Consistency

How to measure internal consistency:

- [Cronbach's Alpha (&alpha;)]({{< var ref-reliability.cronbachs-alpha >}}) 

Interpretation

- You want internal consistency results (i.e. [Cronbach's Alpha (&alpha;)]({{< var ref-reliabiity.cronbachs-alpha >}})) to be high, we do not want it to be too high.
- An (&alpha;) greater than about 0.90 may indicate unnecessary redundancy among the items and means that some of them can be eliminated without jeopardizing the reliability of the scale.

:::{.callout title="What if my scale has low internal consistency?"}
If the scale you are using has already been developed and validated, you cannot change much with respect to internal consistency

- Changing the items makes it impossible to compare to studies on its psychometric validity
:::

# Reliability Coefficients

Common reliability statistics include:

- [Percent agreement]({{< var ref-reliability.percent-agreement >}})
- [Kappa Coefficient]({{< var ref-reliability.kappa-coefficient >}})
- [Cronbach's Alpha]({{< var ref-reliability.cronbachs-alpha >}})
- [Correlation Coefficient]({{< var ref-correlation-coefficient.path >}})
- [Intra-Class Correlation Coefficient]({{< var ref-reliability.icc >}})

# Calculation

Reliability is the proportion of the total variance in scores that is due to differences among people @streinerReliabilityStatisticsCommentarySeries2016a.

$$
Reliability = \frac{\sigma_s^2}{\sigma_s^2 + \sigma_e^2} = \frac{\sigma_s^2}{\sigma_Total^2}
$$

- $\sigma_s^2$ refers to the variability between subjects @streinerReliabilityStatisticsCommentarySeries2016a
- $\sigma_e^2$ refers to measurement error @streinerReliabilityStatisticsCommentarySeries2016a


:::{.callout-note}
- Terms such as ‘precision,’‘agreement,’ and ‘repeatability’ focus solely on the fact that the error term should be as small as possible @streinerReliabilityStatisticsCommentarySeries2016a.
- These terms ignore the fact that reliability also depends on the variability among people @streinerReliabilityStatisticsCommentarySeries2016a.
:::

:::{.callout-note title="Subject variability implications"}

- If there is no variability between participants ( $\sigma_s^2$ ) then $\sigma_s^2 = 0$ the reliability is 0 @streinerReliabilityStatisticsCommentarySeries2016a.
    - The reliability of a scale reflects its ability to differentiate among people and if it cannot, then the reliability is 0 and the scale is useless @streinerReliabilityStatisticsCommentarySeries2016a.
- Another implication about $\sigma_s^2$ is that reliability is not a fixed property, but rather dependent on the sample being studied.
    - Reliability is is very dependent on the sample in which it is determined @streinerReliabilityStatisticsCommentarySeries2016a.
    - Example: Applying a depression scale to ER patients vs outpatient psychiatric patients
        - The ER Patients would consist of people who have very low depression to extremely high depression (suicidally depressed) @streinerReliabilityStatisticsCommentarySeries2016a.
            - This sample has a very high patient variability ( $\sigma_s^2$ ) and thus a higher reliability score @streinerReliabilityStatisticsCommentarySeries2016a
        - The outpatient psychiatric patients would likely have moderate levels of depression (those with high depression would be administered to a hospital) @streinerReliabilityStatisticsCommentarySeries2016a.
            - Thus there will be low patient variability ($\sigma_s^2$) and the reliability will be lower @streinerReliabilityStatisticsCommentarySeries2016a.
:::

# Scoring

Values range from 0.00 (not reliable) to 1.00 (perfectly reliable) 

| Score | Reliability |
|-|-|
| 0.00 – 0.20 | Poor |
| 0.21 – 0.40 | Fair |
| 0.41 – 0.60 | Moderate |
| 0.61 – 0.80 | Good |
| 0.81 – 1.00 | Excellent |

: "Rule of thumb" Quick interpretation of reliability {.striped .hover}






# Interpretation

:::{.callout title="How High Should Reliability be?"}
How high should reliability be? 

- Scales used in new, underdeveloped research areas should have a minimum reliability of 0.70 @streinerReliabilityStatisticsCommentarySeries2016a.
- Scales coming from mature areas of research, the minimum is 0.80 @streinerReliabilityStatisticsCommentarySeries2016a.
- and if the scale is to be used for clinical purposes the minimum is 0.90.
:::

*Poor-moderate reliability are not good for clinical decision making since you are likely to get a different result everytime you test your patient, regardless of status
